{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39f3c65",
   "metadata": {},
   "source": [
    "<h1>Creation of phenopackets from tabular data (individuals in columns)</h1>\n",
    "<p>We will process <a href=\"https://pubmed.ncbi.nlm.nih.gov/34521999/\" target=\"__blank\">`Dingemans, et al. (2022) Establishing the phenotypic spectrum of ZTTK syndrome by analysis of 52 individuals with variants in SON</a></p>\n",
    "<p>pyphetools provides a convenient way of extracting HPO terms from typical tables presented in supplemental material. Typical tables can have the individuals in columns or rows. In this case, we extract data from TABLE.</p>\n",
    "<p>This note shows how to work through the table and set up the pyphetools encoder. The table was not originally available in the table, but constructed using the data in the publication</p>\n",
    "<p>Users can work on one column at a time and then generate a collection of <a href=\"https://pubmed.ncbi.nlm.nih.gov/35705716/\" target=\"__blank\">GA4GH phenopackets</a> to represent each patient included in the original supplemental material. These phenopackets can then be used for a variety of downstream applications.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0602c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_separate_hpos_from_df' from 'pyphetools.creation.simple_column_mapper' (C:\\Users\\z184215\\Documents\\H_schijf_Lex\\Projecten\\HPO_clustering_Peter\\Scripts\\pyphetools\\pyphetools\\creation\\simple_column_mapper.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../pyphetools\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyphetools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyphetools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimple_column_mapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m try_mapping_columns, get_separate_hpos_from_df\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_separate_hpos_from_df' from 'pyphetools.creation.simple_column_mapper' (C:\\Users\\z184215\\Documents\\H_schijf_Lex\\Projecten\\HPO_clustering_Peter\\Scripts\\pyphetools\\pyphetools\\creation\\simple_column_mapper.py)"
     ]
    }
   ],
   "source": [
    "import phenopackets as php\n",
    "from google.protobuf.json_format import MessageToDict, MessageToJson\n",
    "from google.protobuf.json_format import Parse, ParseDict\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # show entire column contents, important!\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../../pyphetools'))\n",
    "sys.path.insert(0, os.path.abspath('../../pyphetools'))\n",
    "from pyphetools.creation import *\n",
    "from pyphetools.creation.simple_column_mapper import try_mapping_columns, get_separate_hpos_from_df\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606e7eb",
   "metadata": {},
   "source": [
    "<h2>Importing HPO data</h2>\n",
    "<p>pyphetools uses the Human Phenotype Ontology (HPO) to encode phenotypic features. The recommended way of doing this is to ingest the hp.json file using HpoParser, which in turn creates an HpoConceptRecognizer object. </p>\n",
    "<p>The HpoParser can accept a hpo_json_file argument if you want to use a specific file. If the argument is not passed, it will download the latext hp.json file from the HPO GitHub site and store it in a new subdirectory called hpo_data. It will not download the file if the file is already downloaded.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7789fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HpoParser()\n",
    "hpo_cr = parser.get_hpo_concept_recognizer()\n",
    "hpo_version = parser.get_version()\n",
    "metadata = MetaData(created_by=\"ORCID:0000-0002-5648-2155\")\n",
    "metadata.default_versions_with_hpo(version=hpo_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102722a",
   "metadata": {},
   "source": [
    "<h2>Importing the supplemental table</h2>\n",
    "<p>Here, we use the pandas library to import this file (note that the Python package called openpyxl must be installed to read Excel files with pandas, although the library does not need to be imported in this notebook). pyphetools expects a pandas DataFrame as input, and users can choose any input format available for pandas include CSV, TSV, and Excel, or can use any other method to transform their input data into a Pandas DataFrame before using pyphetools.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../../data/SON/PMID_34521999.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7bad9",
   "metadata": {},
   "source": [
    "<h1>Converting to row-based format</h1>\n",
    "<p>To use pyphetools, we need to have the individuals represented as rows (one row per individual) and have the items of interest be encoded as column names. The required transformations for doing this may be different for different input data, but often we will want to transpose the table (using the pandas <tt>transpose</tt> function) and set the column names of the new table to the zero-th row. After this, we drop the zero-th row (otherwise, it will be interpreted as an individual by the pyphetools code).</p>\n",
    "<p>After this step is completed, the remaining steps to create phenopackets are the same as in the \n",
    "    <a href=\"http://localhost:8888/notebooks/notebooks/Create%20phenopackets%20from%20tabular%20data%20with%20individuals%20in%20rows.ipynb\" target=\"__blank\">row-based notebook</a>.</p>\n",
    "    \n",
    "Furthermore, for this specific case, there is a Count features row that we want dropped, so we filter out any row that does not have Patient in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.transpose()\n",
    "dft.columns = dft.iloc[0]\n",
    "dft.drop(dft.index[0], inplace=True)\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013666d",
   "metadata": {},
   "source": [
    "Some column names might include spaces in front or after, and a couple of columns are subheadings and only contain NaNs, so lets correct that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d604600",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.columns = dft.columns.str.strip()\n",
    "dft = dft.dropna(axis=1, how='all')\n",
    "dft['patient_id'] = dft.index\n",
    "dft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314d6bd",
   "metadata": {},
   "source": [
    "<h2>Column mappers</h2>\n",
    "<p>Please see the notebook \"Create phenopackets from tabular data with individuals in rows\" for explanations. In the following cell we create a dictionary for the ColumnMappers. Note that the code is identical except that we use the df.loc function to get the corresponding row data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_cr = parser.get_hpo_concept_recognizer()\n",
    "column_mapper_d, col_not_found = try_mapping_columns(df=dft,\n",
    "                                                    observed='+',\n",
    "                                                    excluded='-',\n",
    "                                                    hpo_cr=hpo_cr,\n",
    "                                                    preview=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122946b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col_not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "headcircumference = {'> P98': 'Macrocephaly at birth',\n",
    "                 '< P3': 'Primary microcephaly',\n",
    "        }\n",
    "headcircumferenceMapper = OptionColumnMapper(concept_recognizer=hpo_cr, option_d=headcircumference)\n",
    "print(headcircumferenceMapper.preview_column(dft['Head circumference (at birth) (HP:0011451 / HP:0004488)']))\n",
    "column_mapper_d['Head circumference (at birth) (HP:0011451 / HP:0004488)'] = headcircumferenceMapper\n",
    "\n",
    "headcircumference = {'> P98': 'Macrocephaly',\n",
    "                 '< P3': 'Microcephaly',\n",
    "        }\n",
    "headcircumferenceMapper = OptionColumnMapper(concept_recognizer=hpo_cr, option_d=headcircumference)\n",
    "print(headcircumferenceMapper.preview_column(dft['Head circumference (HP:0000252 / HP:0000256)']))\n",
    "column_mapper_d['Head circumference (HP:0000252 / HP:0000256)'] = headcircumferenceMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_length = {'> P98': 'Birth length greater than 97th percentile',\n",
    "                 '< P3': 'Birth length less than 3rd percentile',\n",
    "        }\n",
    "birth_lengthMapper = OptionColumnMapper(concept_recognizer=hpo_cr, option_d=birth_length)\n",
    "print(birth_lengthMapper.preview_column(dft['Heigth (at birth) (HP:0003561 / HP:0003517)']))\n",
    "column_mapper_d['Heigth (at birth) (HP:0003561 / HP:0003517)'] = birth_lengthMapper\n",
    "\n",
    "length = {'> P98': 'Tall stature',\n",
    "                 '< P3': 'Short stature',\n",
    "        }\n",
    "lengthMapper = OptionColumnMapper(concept_recognizer=hpo_cr, option_d=length)\n",
    "print(lengthMapper.preview_column(dft['Heigth (HP:0004322 / HP:0000098)']))\n",
    "column_mapper_d['Heigth (HP:0004322 / HP:0000098)'] = lengthMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_weight = {'> P98': 'Large for gestational age',\n",
    "                 '< P3': 'Small for gestational age',\n",
    "        }\n",
    "birth_weightMapper = OptionColumnMapper(concept_recognizer=hpo_cr, option_d=birth_weight)\n",
    "print(birth_weightMapper.preview_column(dft['Weigth (at birth) (HP:0001518 / HP:0001520)']))\n",
    "column_mapper_d['Weigth (at birth) (HP:0001518 / HP:0001520)'] = birth_weightMapper\n",
    "\n",
    "weight = {'> P98': 'Increased body weight',\n",
    "                 '< P3': 'Decreased body weight',\n",
    "        }\n",
    "weightMapper = OptionColumnMapper(concept_recognizer=hpo_cr, option_d=weight)\n",
    "print(weightMapper.preview_column(dft['Weigth (HP:0004325 / HP:0004324)']))\n",
    "column_mapper_d['Weigth (HP:0004325 / HP:0004324)'] = weightMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e52430",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_severity = {'Mild': 'Intellectual disability, mild',\n",
    "                 'Moderate': 'Intellectual disability, moderate',\n",
    "         'Severe': 'Intellectual disability, severe'\n",
    "        }\n",
    "id_severityMapper = OptionColumnMapper(concept_recognizer=hpo_cr, option_d=id_severity)\n",
    "print(id_severityMapper.preview_column(dft['Severity of intellectual disability (HP:0001256 / HP:0002342 / HP:0010864)']))\n",
    "column_mapper_d['Severity of intellectual disability (HP:0001256 / HP:0002342 / HP:0010864)'] = id_severityMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc53600",
   "metadata": {},
   "source": [
    "For this particular file, there are HPO terms in the cells of the table as well, so we should loop them, parse contents and add them to the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_hpos = get_additional_hpos_from_df(dft, hpo_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53617d",
   "metadata": {},
   "source": [
    "<h2>Variant Data</h2>\n",
    "<p>The variant data (HGVS< transcript) is listed in the Variant (hg19, NM_015133.4) column.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = 'hg38'\n",
    "default_genotype = 'heterozygous'\n",
    "transcript='NM_138927.2'\n",
    "varMapper = VariantColumnMapper(assembly=genome,column_name='cDNA change', \n",
    "                                transcript=transcript, genotype=default_genotype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af2052",
   "metadata": {},
   "source": [
    "<h1>Demographic data</h1>\n",
    "<p>pyphetools can be used to capture information about age, sex, and individual identifiers. This information is stored in a map of \"IndividualMapper\" objects. Special treatment may be required for the indifiers, which may be used as the column names or row index.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ageMapper = AgeColumnMapper.by_year('Age at examination')\n",
    "ageMapper.preview_column(dft['Age at examination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f664cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sexMapper = SexColumnMapper(male_symbol='Male', female_symbol='Female', column_name='Gender')\n",
    "sexMapper.preview_column(dft['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6581a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid = \"PMID: 34521999\"\n",
    "encoder = CohortEncoder(df=dft, hpo_cr=hpo_cr, column_mapper_d=column_mapper_d, \n",
    "                        individual_column_name=\"patient_id\", agemapper=ageMapper, sexmapper=sexMapper,\n",
    "                       variant_mapper=varMapper, metadata=metadata,\n",
    "                       pmid=pmid)\n",
    "encoder.set_disease(disease_id='617140', label='ZTTK SYNDROME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd367ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = encoder.get_individuals(additional_hpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d044b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = individuals[0]\n",
    "phenopacket1 = i1.to_ga4gh_phenopacket(metadata=metadata.to_ga4gh())\n",
    "json_string = MessageToJson(phenopacket1)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"../../phenopackets/SON/\"\n",
    "encoder.output_phenopackets(outdir=output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
